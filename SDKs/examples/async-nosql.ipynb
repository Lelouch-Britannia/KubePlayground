{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d534d3",
   "metadata": {},
   "source": [
    "# NoSQL DB With Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26fb35f",
   "metadata": {},
   "source": [
    "## 1. Configuration Setup & Connection Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f531010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pprint\n",
    "import logging\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from typing import List, Optional, Dict, Any, Type, TypeVar\n",
    "from datetime import datetime, timezone\n",
    "from beanie import Document\n",
    "from daolib.drivers.nosql.mongo_connector import MongoConnector\n",
    "from daolib.drivers.nosql.config import NoSQLConnectionEntry\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e2e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- YAML File Reader ----\n",
    "\n",
    "class YamlFileOperator:\n",
    "    \"\"\"Simple YAML file reader\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def read(file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Read YAML file and return the parsed content\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                return yaml.safe_load(file)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Config file not found: {file_path}\")\n",
    "        except yaml.YAMLError as e:\n",
    "            raise ValueError(f\"Error parsing YAML file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ee874",
   "metadata": {},
   "source": [
    "### 1.1 Configuration Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Configuration Constants ----\n",
    "\n",
    "class Constants:\n",
    "    \n",
    "    class DBConstants:\n",
    "        \"\"\"Database-related constants\"\"\"\n",
    "        \n",
    "        # YAML structure keys\n",
    "        nosql_creds = \"nosql_creds\"\n",
    "        mongo_inst = \"mongo_inst\"\n",
    "        username = \"username\"\n",
    "        host = \"host\"\n",
    "        port = \"port\"\n",
    "        db_name = \"db_name\"\n",
    "        min_pool_size = \"min_pool_size\"\n",
    "        max_pool_size = \"max_pool_size\"\n",
    "        use_srv = \"use_srv\" # Added to constants\n",
    "        # Passwords from environment variables (not stored in YAML for security)\n",
    "        mongo_password_dev = os.getenv(\"MONGO_PASSWORD_DEV\", \"password\")\n",
    "        mongo_password_prod = os.getenv(\"MONGO_PASSWORD_PROD\", \"password\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6627f4a9",
   "metadata": {},
   "source": [
    "### 1.2 Custom MongoConnector Helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e15eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Optional, List, Any\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MongoHelper(MongoConnector):\n",
    "    \"\"\"\n",
    "    Concrete implementation of MongoConnector.\n",
    "    Loads configuration from a YAML file (injected or default) and Env variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: Optional[str] = None, document_models: Optional[List[Any]] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config_path: Optional path to YAML config. If None, defaults to config/{env}.yml\n",
    "            document_models: List of Beanie Document classes to register.\n",
    "        \"\"\"\n",
    "        # 1. Initialize Parent (Registers models and sets up locks)\n",
    "        super().__init__(document_models=document_models)\n",
    "\n",
    "        # 2. Singleton Guard: Only setup config path if not already set\n",
    "        # This prevents overwriting if 'init()' is called multiple times on the singleton\n",
    "        if not hasattr(self, \"_config_path\"):\n",
    "            \n",
    "            # Capture the environment context (e.g., 'development', 'production')\n",
    "            self._env = os.getenv('ENVIRONMENT', 'development')\n",
    "\n",
    "            if config_path:\n",
    "                self._config_path = config_path\n",
    "            else:\n",
    "                # Default logic: Use project_root/config/{env}.yml\n",
    "                # Adjust 'current_dir' logic as needed for your project structure\n",
    "                current_dir = os.getcwd() \n",
    "                self._config_path = os.path.abspath(\n",
    "                    os.path.join(current_dir, \"config\", f\"{self._env}.yml\")\n",
    "                )\n",
    "            \n",
    "            logger.info(f\"MongoHelper configured using path: {self._config_path}\")\n",
    "\n",
    "    def read_and_load_config(self) -> NoSQLConnectionEntry:\n",
    "        \"\"\"\n",
    "        Reads the YAML file stored in self._config_path and merges with ENV vars.\n",
    "        \"\"\"\n",
    "        # 1. Read YAML\n",
    "        try:\n",
    "            # Verify file exists before trying to read\n",
    "            if not os.path.exists(self._config_path):\n",
    "                raise FileNotFoundError(f\"Config file not found at: {self._config_path}\")\n",
    "                \n",
    "            configs_data = YamlFileOperator.read(self._config_path)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to read Mongo config: {e}\")\n",
    "            raise e\n",
    "        \n",
    "        # 2. Extract Config Section (Safe navigation)\n",
    "        # Adjust keys based on your actual YAML structure\n",
    "        nosql_section = configs_data.get(Constants.DBConstants.nosql_creds, {})\n",
    "        mongo_data = nosql_section.get(Constants.DBConstants.mongo_inst, {})\n",
    "        \n",
    "        # 3. Determine Password based on Environment\n",
    "        # It's cleaner to use a single standard ENV var (MONGO_PASSWORD), \n",
    "        # but we support your split dev/prod logic here:\n",
    "        if self._env == \"development\":\n",
    "            password = os.getenv(\"MONGO_PASSWORD_DEV\", \"password\")\n",
    "        else:\n",
    "            password = os.getenv(\"MONGO_PASSWORD_PROD\", \"\")\n",
    "            \n",
    "        # 4. Return the Data Class\n",
    "        return NoSQLConnectionEntry(\n",
    "            username=mongo_data.get(Constants.DBConstants.username, \"\"),\n",
    "            password=password,\n",
    "            host=mongo_data.get(Constants.DBConstants.host, \"localhost\"),\n",
    "            port=int(mongo_data.get(Constants.DBConstants.port, 27017)),\n",
    "            database=mongo_data.get(Constants.DBConstants.db_name, \"development\"),\n",
    "            min_pool_size=int(mongo_data.get(Constants.DBConstants.min_pool_size, 10)),\n",
    "            max_pool_size=int(mongo_data.get(Constants.DBConstants.max_pool_size, 50)),\n",
    "            use_srv=mongo_data.get(Constants.DBConstants.use_srv, False),\n",
    "            use_ssl=mongo_data.get(\"use_ssl\", False) # Added SSL support if in YAML\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8480fadc",
   "metadata": {},
   "source": [
    "### 1.3 Document Models\n",
    "\n",
    "We define Beanie `Document` models for our data entities. These models automatically integrate with MongoDB through the initialized connector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bece380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Define Beanie Document Models ----\n",
    "\n",
    "class Address(Document):\n",
    "    \"\"\"Address document for embedding or referencing\"\"\"\n",
    "    street: str\n",
    "    number: int\n",
    "    city: str\n",
    "    country: str\n",
    "    zip: str\n",
    "    owner_id: Optional[str] = None\n",
    "    \n",
    "    class Settings:\n",
    "        collection = \"addresses\"\n",
    "\n",
    "\n",
    "class Person(Document):\n",
    "    \"\"\"Person document with optional embedded addresses\"\"\"\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    age: int\n",
    "    \n",
    "    class Settings:\n",
    "        collection = \"person_collection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69face4",
   "metadata": {},
   "source": [
    "### 1.4 Initialize Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23331dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Initialize Connector ----\n",
    "D = TypeVar(\"D\", bound=Document)\n",
    "\n",
    "async def setup_async_connection(\n",
    "        models: List[Type[D]], \n",
    "        config_path: Optional[str] = None\n",
    "    ) -> MongoConnector:\n",
    "    \n",
    "    \"\"\"Initialize the async MongoDB connection with daolib using YAML config\"\"\"\n",
    "    connector = MongoHelper(document_models=models)\n",
    "    await connector.init()\n",
    "    print(\"✓ Async MongoDB connection established via daolib\")\n",
    "    return connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ec43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "printer = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# Initialize the async connection\n",
    "connector = await setup_async_connection(models=[Address, Person])\n",
    "\n",
    "# Only for local development\n",
    "await Person.find({}).delete()\n",
    "await Address.find({}).delete()\n",
    "print(\"✓ Database cleared for fresh start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc3afb",
   "metadata": {},
   "source": [
    "## 2. Async CRUD Operations with daolib\n",
    "\n",
    "This section demonstrates CRUD (Create, Read, Update, Delete) operations using daolib's `MongoConnector` and Beanie for async operations with MongoDB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98173e06",
   "metadata": {},
   "source": [
    "### 2.1 Insert Operations\n",
    "\n",
    "Create and insert sample data into the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and insert persons using Beanie\n",
    "async def create_persons_async(\n",
    "    first_names: List[str],\n",
    "    last_names: List[str],\n",
    "    ages: List[int]\n",
    ") -> List[str]:\n",
    "    \"\"\"Insert multiple persons and return their IDs\"\"\"\n",
    "    persons = []\n",
    "    \n",
    "    for first_name, last_name, age in zip(first_names, last_names, ages):\n",
    "        person = Person(\n",
    "            first_name=first_name,\n",
    "            last_name=last_name,\n",
    "            age=age\n",
    "        )\n",
    "        persons.append(person)\n",
    "    \n",
    "    # Beanie handles bulk insert\n",
    "    results = await Person.insert_many(persons)\n",
    "    # person_ids = [str(result.id) for result in results]\n",
    "    # return person_ids\n",
    "    return results.inserted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda5e1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert sample data\n",
    "first_names = [\"Rahul\", \"Ananya\", \"Vikram\", \"Priya\", \"Arjun\"]\n",
    "last_names = [\"Sharma\", \"Gupta\", \"Singh\", \"Mehta\", \"Verma\"]\n",
    "ages = [28, 24, 32, 27, 35]\n",
    "\n",
    "inserted_ids_async = await create_persons_async(first_names, last_names, ages)\n",
    "print(f\"✓ Inserted {len(inserted_ids_async)} persons\")\n",
    "print(f\"IDs: {inserted_ids_async}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be47af97",
   "metadata": {},
   "source": [
    "### 2.3 Read Operations\n",
    "\n",
    "Link: [Documentation](https://beanie-odm.dev/tutorial/finding-documents/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24590dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all (Generic)\n",
    "T = TypeVar(\"T\", bound=Document)\n",
    "\n",
    "async def find_all(doc: Type[T]):\n",
    "    \"\"\"Retrieve all persons\"\"\"\n",
    "    persons = await doc.find().to_list()\n",
    "    for person in persons:\n",
    "        # Retrieve FULL object, but exclude ID during dump\n",
    "        # This works for ANY document without needing extra classes\n",
    "        printer.pprint(person.model_dump(exclude={\"id\", \"revision_id\"}))\n",
    "\n",
    "await find_all(Person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaaeb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all with projection (Non generic)\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ViewModel(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    age: int\n",
    "\n",
    "persons = await Person.find({}).project(ViewModel).to_list()\n",
    "for person in persons:\n",
    "    printer.pprint(person.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4657cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get by ID (Generic)\n",
    "T = TypeVar(\"T\", bound=Document)\n",
    "\n",
    "async def get_by_id(doc: Type[T], _id: str):\n",
    "    \"\"\"Retrieve a person by ID\"\"\"\n",
    "    # Convert string ID back to ObjectId\n",
    "    from bson import ObjectId\n",
    "    result = await doc.get(ObjectId(_id))\n",
    "    if result:\n",
    "        printer.pprint(result.model_dump(exclude={\"id\", \"revision_id\"}))\n",
    "    else:\n",
    "        print(f\"No person found with ID: {_id}\")\n",
    "\n",
    "\n",
    "# Retrieve the first person\n",
    "print(f\"Fetching person with ID: {inserted_ids_async[0]}\")\n",
    "await get_by_id(Person, inserted_ids_async[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b94d812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search in range using Beanie query syntax\n",
    "class ViewModel(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    age: int\n",
    "\n",
    "persons = await Person.find(\n",
    "    Person.age >= 25,\n",
    "    Person.age <= 35\n",
    ").sort(\"+age\").project(ViewModel).to_list()\n",
    "\n",
    "for person in persons:\n",
    "    printer.pprint(person.model_dump())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a6c01",
   "metadata": {},
   "source": [
    "### 2.4 Update Operations\n",
    "\n",
    "Link: [Documentation](https://beanie-odm.dev/tutorial/updating-%26-deleting/)\n",
    "\n",
    "Treating your database as a **storage bucket** (PyMongo style) versus treating it as a **collection of Python objects** (Beanie/ODM style).\n",
    "\n",
    "#### 1. Data Integrity & Validation\n",
    "\n",
    "When you use `.update()`, you are bypassing your application's validation logic. You could technically insert \"bad\" data that doesn't match your Pydantic model, and you wouldn't know until you tried to read it back and your app crashed.\n",
    "\n",
    "* `.save()`/`.replace()`: Runs all Pydantic validators. If you try to save an invalid email or a negative age, Python throws an error *before* the bad data hits the database.\n",
    "\n",
    "\n",
    "* `.update()`: Sends the command directly to MongoDB. MongoDB checks types roughly, but it doesn't know your Pydantic \"business rules.\"\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "# Bad update: MongoDB accepts this, but it breaks your Pydantic model later\n",
    "await User.find_one(User.id == doc_id).update({\"$set\": {\"email\": \"not-an-email\"}}) \n",
    "\n",
    "# Good save: This raises a ValidationError immediately\n",
    "user = await User.get(doc_id)\n",
    "user.email = \"not-an-email\"\n",
    "await user.save() # BOOM! Protected.\n",
    "\n",
    "```\n",
    "\n",
    "#### 2. Lifecycle Events (Hooks)\n",
    "\n",
    "Beanie supports event-based actions (hooks) like `before_save`, `after_save`, `before_replace`, etc..\n",
    "\n",
    "* **`.save()`:** Triggers these hooks. For example, if you have logic to automatically update an `updated_at` timestamp or hash a password whenever a user is saved, `.save()` ensures this happens.\n",
    "* **`.update()`:** Bypasses these hooks entirely.\n",
    "\n",
    "#### 3. Complex Business Logic\n",
    "\n",
    "Sometimes the \"new value\" isn't a simple increment (`$inc`) or set (`$set`). It might require complex Python calculations, external API calls, or logic that is painful to write in MongoDB Query Language (MQL).\n",
    "\n",
    "**Scenario:** Calculate a \"trust score\" based on 10 different fields and an external credit check.\n",
    "\n",
    "* **Efficient:** Fetch the doc  Run complex Python math  `.save()`.\n",
    "* **Hard:** trying to write a MongoDB aggregation pipeline update to do that math inside the database.\n",
    "\n",
    "#### 4. Semantic \"PUT\" Operations\n",
    "\n",
    "In REST API design:\n",
    "\n",
    "* **PATCH** (`.update()`): \"Change just these specific fields.\"\n",
    "* **PUT** (`.replace()`): \"Here is the **new complete state** of the resource. Make the database match this exactly.\"\n",
    "\n",
    "If you are building an API endpoint that receives a full user profile form and saves it, `.replace()` guarantees that the database matches exactly what the user submitted, removing any old fields they might have cleared out.\n",
    "\n",
    "#### Summary: When to use which?\n",
    "\n",
    "| Feature | Use `.save()` / `.replace()` | Use `.update()` |\n",
    "| --- | --- | --- |\n",
    "| **Data Safety** | **High.** Enforces Pydantic schema & validation. | **Low.** Bypasses app validation. |\n",
    "| **Logic** | **Complex.** Good for heavy Python calculations. | **Simple.** Good for increments, flags, renames. |\n",
    "| **Side Effects** | **Triggers hooks** (e.g., `updated_at`, password hashing). | **Silent.** No Python hooks triggered. |\n",
    "| **Performance** | **Slower.** Full document read/write roundtrip. | **Faster.** Single DB operation. |\n",
    "\n",
    "#### Recommendation\n",
    "\n",
    "**Default to `.save()`** for most standard application logic to keep your data safe and your code clean. Optimize with **`.update()`** only when:\n",
    "\n",
    "1. Performance is critical (high-frequency updates).\n",
    "2. You need atomicity (preventing race conditions).\n",
    "3. You are doing simple patch operations (toggling a boolean, incrementing a counter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c8eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update (Non Generic)\n",
    "# Update the second person (Ananya)\n",
    "from bson import ObjectId\n",
    "target_id = inserted_ids_async[1]\n",
    "\n",
    "# Atomic update (Pydantic validation fails)\n",
    "await Person.find_one(Person.id == ObjectId(target_id)).update({    # type: ignore\n",
    "    \"$inc\": {\"age\": 1},    \n",
    "    \"$set\": {\"married\": False},\n",
    "})\n",
    "\n",
    "# Verify the update\n",
    "print(\"\\nVerifying update:\")\n",
    "# Fails the pydantic validation (No married field)\n",
    "await get_by_id(Person, target_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92faed1",
   "metadata": {},
   "source": [
    "### 2.5 Delete Operations\n",
    "\n",
    "Remove documents from the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete by ID (Generic)\n",
    "T = TypeVar(\"T\", bound=Document)\n",
    "\n",
    "async def delete_by_id(doc: Type[T], _id: str):\n",
    "\n",
    "    from bson import ObjectId\n",
    "    result = await doc.get(ObjectId(_id))\n",
    "\n",
    "    if result:\n",
    "        await result.delete()\n",
    "        print(f\"✓ Deleted person {_id}\")\n",
    "    else:\n",
    "        print(f\"Person {_id} not found\")\n",
    "\n",
    "\n",
    "# Delete the third person (Vikram)\n",
    "target_id = inserted_ids_async[2]\n",
    "await delete_by_id(Person, target_id)\n",
    "\n",
    "# Verify deletion\n",
    "print(\"\\nVerifying deletion:\")\n",
    "await get_by_id(Person, target_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e01280b",
   "metadata": {},
   "source": [
    "## Beanie vs PyMongo: Aggregation Syntax Guide\n",
    "\n",
    "**TL;DR:** For complex aggregations (joins, grouping, transformations), Beanie uses the **exact same MongoDB aggregation pipeline syntax as PyMongo**. There are no special Beanie wrapper methods for `$lookup`, `$group`, `$project`, etc.\n",
    "\n",
    "### Three Ways to Query in Beanie\n",
    "\n",
    "#### 1. Simple Queries (Beanie Syntax)\n",
    "```python\n",
    "# Find with filters\n",
    "books = await Book.find(Book.type == \"fiction\", Book.copies > 10).to_list()\n",
    "\n",
    "# With projection\n",
    "class SimpleView(BaseModel):\n",
    "    title: str\n",
    "    copies: int\n",
    "\n",
    "books = await Book.find(Book.type == \"fiction\").project(SimpleView).to_list()\n",
    "```\n",
    "\n",
    "#### 2. Aggregation Methods (Beanie Helpers)\n",
    "```python\n",
    "# Built-in aggregation on query results\n",
    "avg_copies = await Book.find(Book.type == \"fiction\").avg(Book.copies)\n",
    "total_copies = await Book.find().sum(Book.copies)\n",
    "max_price = await Product.find(Product.category == \"Electronics\").max(Product.price)\n",
    "```\n",
    "\n",
    "Available: `.sum()`, `.avg()`, `.max()`, `.min()`\n",
    "\n",
    "#### 3. Complex Aggregation Pipelines (Raw PyMongo Syntax)\n",
    "\n",
    "**This is where confusion happens!** Beanie does NOT provide wrapper methods for pipeline stages.\n",
    "\n",
    "```python\n",
    "# ✅ CORRECT: Use raw MongoDB operators in a list of dictionaries\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"author\",\n",
    "        \"localField\": \"author_id\", \n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"author_details\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$author_details\"},\n",
    "    {\"$group\": {\n",
    "        \"_id\": \"$type\",\n",
    "        \"total_books\": {\"$sum\": 1},\n",
    "        \"avg_copies\": {\"$avg\": \"$copies\"}\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"type\": \"$_id\",\n",
    "        \"total_books\": 1,\n",
    "        \"avg_copies\": 1\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = await Book.aggregate(pipeline).to_list()\n",
    "```\n",
    "\n",
    "```python\n",
    "# ❌ WRONG: There is no Beanie method like .lookup() or .group()\n",
    "# This does NOT exist:\n",
    "results = await Book.lookup(\"author\").group(\"type\").to_list()  # INVALID\n",
    "```\n",
    "\n",
    "### Key Differences Summary\n",
    "\n",
    "| Feature | Beanie | PyMongo |\n",
    "|---------|--------|---------|\n",
    "| **Simple find** | `await Book.find(Book.price > 10).to_list()` | `await collection.find({\"price\": {\"$gt\": 10}}).to_list()` |\n",
    "| **Aggregation helpers** | `await Book.find().avg(Book.price)` | Manual aggregation pipeline |\n",
    "| **Complex pipelines** | `await Book.aggregate([{...}]).to_list()` | `await collection.aggregate([{...}]).to_list()` |\n",
    "| **Pipeline syntax** | **Same raw dictionaries** | **Same raw dictionaries** |\n",
    "| **Result parsing** | Can use `projection_model=MyModel` | Returns dicts |\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "- **Use `.find()` queries**: Simple filtering, sorting, limiting\n",
    "- **Use aggregation helpers**: Quick calculations (avg, sum, max, min)\n",
    "- **Use `.aggregate()` with pipelines**: Joins, grouping, complex transformations, multi-stage operations\n",
    "\n",
    "**References:**\n",
    "- [Beanie Aggregation Tutorial](https://beanie-odm.dev/tutorial/aggregation/)\n",
    "- [MongoDB Aggregation Pipeline](https://www.mongodb.com/docs/manual/core/aggregation-pipeline/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849d11eb",
   "metadata": {},
   "source": [
    "## 3. Data Relationships\n",
    "\n",
    "Explore different patterns for managing relationships between entities in MongoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a62057",
   "metadata": {},
   "source": [
    "### 3.1 Embedding (One-to-Few)\n",
    "\n",
    "Store related data directly within a parent document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f98516",
   "metadata": {},
   "source": [
    "#### 3.1.1 Define Models for Authors and Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97529655",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Author(Document):\n",
    "    \"\"\"Author document\"\"\"\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    date_of_birth: datetime\n",
    "    \n",
    "    class Settings:\n",
    "        collection = \"author\"\n",
    "\n",
    "class Book(Document):\n",
    "    \"\"\"Book document with embedded authors list\"\"\"\n",
    "    title: str\n",
    "    authors: List[Author] \n",
    "    publish_date: datetime\n",
    "    type: str   # Enum: fiction or non-fiction\n",
    "    copies: int\n",
    "    \n",
    "    class Settings:\n",
    "        collection = \"book\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4120856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register additional models dynamically (dev/test only)\n",
    "await connector.register_models([Author, Book])\n",
    "\n",
    "# Only for local development\n",
    "await Author.find({}).delete()\n",
    "await Book.find({}).delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2adaa0",
   "metadata": {},
   "source": [
    "#### 3.1.2 Insert Author Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f522db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and insert authors\n",
    "authors_data = [\n",
    "    {\n",
    "        \"first_name\": \"Haruki\",\n",
    "        \"last_name\": \"Murakami\",\n",
    "        \"date_of_birth\": datetime(1949, 1, 12, tzinfo=timezone.utc),\n",
    "    },\n",
    "    {\n",
    "        \"first_name\": \"Chimamanda\",\n",
    "        \"last_name\": \"Ngozi Adichie\",\n",
    "        \"date_of_birth\": datetime(1977, 9, 15, tzinfo=timezone.utc),\n",
    "    },\n",
    "    {\n",
    "        \"first_name\": \"Yuval\",\n",
    "        \"last_name\": \"Noah Harari\",\n",
    "        \"date_of_birth\": datetime(1976, 2, 24, tzinfo=timezone.utc),\n",
    "    },\n",
    "]\n",
    "\n",
    "# Insert authors\n",
    "authors = [Author(**data) for data in authors_data]\n",
    "results = await Author.insert_many(authors)\n",
    "author_ids = [str(uid) for uid in results.inserted_ids]\n",
    "\n",
    "print(f\"✓ Inserted {len(author_ids)} authors\")\n",
    "print(f\"Author IDs: {author_ids}\")\n",
    "\n",
    "murakami, adichie, harari = authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f2319",
   "metadata": {},
   "source": [
    "#### 3.1.3 Insert Book Data with Author References\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee4d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and insert books with author references\n",
    "books_data = [\n",
    "    {\n",
    "        \"title\": \"Kafka on the Shore\",\n",
    "        \"authors\": [murakami],\n",
    "        \"publish_date\": datetime(2002, 9, 12, tzinfo=timezone.utc),\n",
    "        \"type\": \"fiction\",\n",
    "        \"copies\": 12,\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Norwegian Wood\",\n",
    "        \"authors\": [murakami],\n",
    "        \"publish_date\": datetime(1987, 9, 4, tzinfo=timezone.utc),\n",
    "        \"type\": \"fiction\",\n",
    "        \"copies\": 9,\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Half of a Yellow Sun\",\n",
    "        \"authors\": [adichie],\n",
    "        \"publish_date\": datetime(2006, 9, 12, tzinfo=timezone.utc),\n",
    "        \"type\": \"fiction\",\n",
    "        \"copies\": 7,\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"We Should All Be Feminists\",\n",
    "        \"authors\": [adichie],\n",
    "        \"publish_date\": datetime(2014, 1, 1, tzinfo=timezone.utc),\n",
    "        \"type\": \"non-fiction\",\n",
    "        \"copies\": 15,\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Sapiens: A Brief History of Humankind\",\n",
    "        \"authors\": [harari],\n",
    "        \"publish_date\": datetime(2011, 1, 1, tzinfo=timezone.utc),\n",
    "        \"type\": \"non-fiction\",\n",
    "        \"copies\": 20,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Insert books\n",
    "books = [Book(**data) for data in books_data]\n",
    "results = await Book.insert_many(books)\n",
    "book_ids = [str(uid) for uid in results.inserted_ids]\n",
    "\n",
    "print(f\"✓ Inserted {len(book_ids)} books\")\n",
    "print(f\"Book IDs: {book_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a2f02",
   "metadata": {},
   "source": [
    "#### 3.1.4 Query Exercises: Embedded Pattern\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Query nested documents using Beanie's dot notation\n",
    "- Filter books by embedded author properties\n",
    "- Project specific fields from embedded documents\n",
    "- Understand when embedding is efficient vs. when it creates duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ebdfa2",
   "metadata": {},
   "source": [
    "**Note on Aggregation Syntax:**\n",
    "\n",
    "For complex transformations, we use **raw PyMongo aggregation pipeline syntax**. Beanie's `.aggregate()` method accepts a list of pipeline stage dictionaries:\n",
    "\n",
    "```python\n",
    "pipeline = [\n",
    "    {\"$project\": {\"field\": 1}},\n",
    "    {\"$group\": {\"_id\": \"$category\"}}\n",
    "]\n",
    "results = await Book.aggregate(pipeline).to_list()\n",
    "```\n",
    "\n",
    "This is the standard MongoDB approach - Beanie does not provide wrapper methods for pipeline stages.\n",
    "\n",
    "**Version Compatibility:** This notebook requires **Beanie >=1.26.0** for proper Motor 3.7+ support. If using Beanie 2.0.1 with Motor 3.7+, aggregation calls will fail with cursor-related errors. See Exercise 3 for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb67bb",
   "metadata": {},
   "source": [
    "**Exercise 1: Find Books by Embedded Author Name**\n",
    "\n",
    "Write a query to find all books where any author's `first_name` is \"Haruki\".\n",
    "\n",
    "**Expected Result:** List of books (Kafka on the Shore, Norwegian Wood)\n",
    "\n",
    "**Hints:**\n",
    "- Use `Book.find()` with query conditions\n",
    "- Access nested fields with dot notation: `Book.authors.first_name`\n",
    "- Use `.to_list()` to execute the query\n",
    "- The `authors` field is a list, so MongoDB will match if ANY element matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4bc377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Find books by embedded author name\n",
    "# TODO: Implement query using Beanie syntax\n",
    "\n",
    "results = await Book.find(\n",
    "    Book.authors.first_name == \"Haruki\" # type: ignore\n",
    ").to_list()\n",
    "\n",
    "printer.pprint([result.model_dump(exclude={\"id\"}) for result in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a99b01",
   "metadata": {},
   "source": [
    "**Exercise 2: Query by Embedded Author Birth Year**\n",
    "\n",
    "Find all books written by authors born after 1970, using the embedded `date_of_birth` field.\n",
    "\n",
    "**Expected Result:** Books by Adichie and Harari\n",
    "\n",
    "**Hints:**\n",
    "- Use `Book.find()` with date comparison\n",
    "- Access nested date field: `Book.authors.date_of_birth`\n",
    "- Use comparison operators: `>` for after\n",
    "- Create a datetime object for comparison: `datetime(1970, 1, 1, tzinfo=timezone.utc)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd70943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Query by embedded author birth year\n",
    "# TODO: Filter using date comparison on nested field\n",
    "\n",
    "results = await Book.find(\n",
    "    Book.authors.date_of_birth > datetime(1970, 1, 1, tzinfo=timezone.utc) # type: ignore\n",
    ").to_list()\n",
    "\n",
    "printer.pprint([result.model_dump(exclude={\"id\"}) for result in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc96fd8",
   "metadata": {},
   "source": [
    "**Exercise 3: Project Partial Embedded Data**\n",
    "\n",
    "Retrieve all books but only include the book title and the author names (not full Author objects).\n",
    "\n",
    "**Expected Output Structure:**\n",
    "```json\n",
    "{\n",
    "  \"title\": \"Kafka on the Shore\",\n",
    "  \"author_names\": [\"Haruki Murakami\"]\n",
    "}\n",
    "```\n",
    "\n",
    "**Hints:**\n",
    "- Use `Book.aggregate()` for complex transformations\n",
    "- Pipeline stages needed: `$project` with `$map` to transform authors array\n",
    "- `$map` syntax: `{\"$map\": {\"input\": \"$authors\", \"in\": \"$$this.first_name\"}}`\n",
    "- Concatenate first and last name using `$concat`\n",
    "- Beanie method: `await Book.aggregate([...]).to_list()`\n",
    "\n",
    "---\n",
    "\n",
    "**⚠️ Version Compatibility Note:**\n",
    "\n",
    "If you encounter this error:\n",
    "```\n",
    "TypeError: object AsyncIOMotorLatentCommandCursor can't be used in 'await' expression\n",
    "```\n",
    "\n",
    "This indicates a **Beanie + Motor version incompatibility**:\n",
    "\n",
    "| Beanie Version | Motor Version | Status |\n",
    "|----------------|---------------|--------|\n",
    "| **2.0.1** | **3.7+** | ❌ **Broken** - Double-await issue |\n",
    "| **>=1.26.0** | **>=3.7.0** | ✅ **Works** - Recommended |\n",
    "| **2.0.1** | **<3.6.0** | ✅ Works - But misses Motor improvements |\n",
    "\n",
    "**Solution:** Upgrade Beanie to fix the issue:\n",
    "```bash\n",
    "pip install --upgrade \"beanie>=1.26.0\"\n",
    "```\n",
    "\n",
    "**Why this happens:** Motor 3.7+ changed cursor behavior. Beanie 2.0.1 tries to `await` the cursor twice, causing the error. Beanie 1.26.0+ has the fix.\n",
    "\n",
    "**References:**\n",
    "- [Beanie Changelog](https://github.com/roman-right/beanie/blob/main/CHANGELOG.md)\n",
    "- [Motor 3.7 Release Notes](https://motor.readthedocs.io/en/stable/changelog.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Project partial embedded data\n",
    "# TODO: Use aggregation to reshape output\n",
    "class OutputModel(BaseModel):\n",
    "    title: str\n",
    "    author_names: List[str]\n",
    "\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"title\": 1,\n",
    "            \"author_names\": {\n",
    "                \"$map\": {\n",
    "                    \"input\": \"$authors\",\n",
    "                    \"as\": \"author\",\n",
    "                    \"in\": {\n",
    "                        \"$concat\": [\n",
    "                            \"$$author.first_name\",\n",
    "                            \" \",\n",
    "                            \"$$author.last_name\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Use the correct Beanie API - get_pymongo_collection() is synchronous\n",
    "# collection = Book.get_pymongo_collection()\n",
    "# results_raw = []\n",
    "# async for doc in collection.aggregate(pipeline):\n",
    "#     results_raw.append(doc)\n",
    "\n",
    "# # Parse into OutputModel\n",
    "# results = [OutputModel(**doc) for doc in results_raw]\n",
    "\n",
    "# printer.pprint([result.model_dump() for result in results])\n",
    "\n",
    "results = await Book.aggregate(pipeline, projection_model=OutputModel).to_list()\n",
    "printer.pprint([result.model_dump() for result in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188806b9",
   "metadata": {},
   "source": [
    "**Exercise 4: Count Books by Type with Embedded Author Info**\n",
    "\n",
    "Using Beanie aggregation, group books by `type` (fiction/non-fiction) and include:\n",
    "- Count of books per type\n",
    "- List of unique author names per type\n",
    "\n",
    "**Challenge:** What happens if the same author appears in multiple books? How do you deduplicate?\n",
    "\n",
    "**Hints:**\n",
    "- Use `Book.aggregate()` with multiple stages\n",
    "- `$unwind` the authors array first to handle embedded documents\n",
    "- `$group` by type field: `{\"$group\": {\"_id\": \"$type\", ...}}`\n",
    "- Use `$addToSet` to collect unique author names (auto-deduplicates)\n",
    "- Use `$sum: 1` to count documents\n",
    "- Final stages: `$project` to rename fields nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Count books by type with author deduplication\n",
    "# TODO: Use aggregation with $group, $addToSet, or $size\n",
    "pipeline = [\n",
    "    # first stage: unwind authors array\n",
    "    {\n",
    "        \"$unwind\": {\n",
    "            \"path\": \"$authors\",\n",
    "            \"preserveNullAndEmptyArrays\": True\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # second stage: group by type and aggregate\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$type\",\n",
    "            \"book_count\": {\"$sum\": 1},\n",
    "            \"total_copies\": {\"$sum\": \"$copies\"},\n",
    "            \"unique_authors\": {\n",
    "                \"$addToSet\": {\n",
    "                    \"$concat\": [\n",
    "                        \"$authors.first_name\",\n",
    "                        \" \",\n",
    "                        \"$authors.last_name\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # third stage: project to rename fields nicely\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"type\": \"$_id\",\n",
    "            \"book_count\": 1,\n",
    "            \"total_copies\": 1,\n",
    "            \"unique_authors\": 1\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "results = await Book.aggregate(pipeline).to_list()\n",
    "printer.pprint(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e4d33",
   "metadata": {},
   "source": [
    "### 3.2 Reference (One-to-Many)\n",
    "\n",
    "Store objectID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af6b2f",
   "metadata": {},
   "source": [
    "#### 3.2.1 Define Models for Authors and Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc982f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bson import ObjectId\n",
    "from beanie import PydanticObjectId\n",
    "\n",
    "class Author2(Document):\n",
    "    \"\"\"Author document\"\"\"\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    date_of_birth: datetime\n",
    "    \n",
    "    class Settings:\n",
    "        collection = \"author2\"\n",
    "\n",
    "class Book2(Document):\n",
    "    \"\"\"Book document with referenced authors using ObjectIds\"\"\"\n",
    "    title: str\n",
    "    authors: List[PydanticObjectId]  # Use Beanie's PydanticObjectId for references\n",
    "    publish_date: datetime\n",
    "    type: str   # Enum: fiction or non-fiction\n",
    "    copies: int\n",
    "    \n",
    "    class Settings:\n",
    "        collection = \"book2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21414ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add models to existing connector\n",
    "await connector.register_models([Author2, Book2])\n",
    "\n",
    "# Only for local development\n",
    "await Author2.find({}).delete()\n",
    "await Book2.find({}).delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c4e67",
   "metadata": {},
   "source": [
    "#### 3.2.2 Insert Author Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22857030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and insert authors\n",
    "authors_data = [\n",
    "    {\n",
    "        \"first_name\": \"Haruki\",\n",
    "        \"last_name\": \"Murakami\",\n",
    "        \"date_of_birth\": datetime(1949, 1, 12, tzinfo=timezone.utc),\n",
    "    },\n",
    "    {\n",
    "        \"first_name\": \"Chimamanda\",\n",
    "        \"last_name\": \"Ngozi Adichie\",\n",
    "        \"date_of_birth\": datetime(1977, 9, 15, tzinfo=timezone.utc),\n",
    "    },\n",
    "    {\n",
    "        \"first_name\": \"Yuval\",\n",
    "        \"last_name\": \"Noah Harari\",\n",
    "        \"date_of_birth\": datetime(1976, 2, 24, tzinfo=timezone.utc),\n",
    "    },\n",
    "]\n",
    "\n",
    "# Insert authors\n",
    "authors = [Author2(**data) for data in authors_data]\n",
    "results = await Author2.insert_many(authors)\n",
    "author_ids: List[PydanticObjectId] = list(results.inserted_ids)\n",
    "\n",
    "print(f\"✓ Inserted {len(author_ids)} authors\")\n",
    "print(f\"Author IDs: {author_ids}\")\n",
    "\n",
    "murakami_id, adichie_id, harari_id = author_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1655065c",
   "metadata": {},
   "source": [
    "#### 3.2.3 Insert Book Data with Author References\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0854f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and insert books with author references\n",
    "books_data = [\n",
    "    {\n",
    "        \"title\": \"Kafka on the Shore\",\n",
    "        \"authors\": [murakami_id],\n",
    "        \"publish_date\": datetime(2002, 9, 12, tzinfo=timezone.utc),\n",
    "        \"type\": \"fiction\",\n",
    "        \"copies\": 12,\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Norwegian Wood\",\n",
    "        \"authors\": [murakami_id],\n",
    "        \"publish_date\": datetime(1987, 9, 4, tzinfo=timezone.utc),\n",
    "        \"type\": \"fiction\",\n",
    "        \"copies\": 9,\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Half of a Yellow Sun\",\n",
    "        \"authors\": [adichie_id],\n",
    "        \"publish_date\": datetime(2006, 9, 12, tzinfo=timezone.utc),\n",
    "        \"type\": \"fiction\",\n",
    "        \"copies\": 7,\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"We Should All Be Feminists\",\n",
    "        \"authors\": [adichie_id],\n",
    "        \"publish_date\": datetime(2014, 1, 1, tzinfo=timezone.utc),\n",
    "        \"type\": \"non-fiction\",\n",
    "        \"copies\": 15,\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Sapiens: A Brief History of Humankind\",\n",
    "        \"authors\": [harari_id],\n",
    "        \"publish_date\": datetime(2011, 1, 1, tzinfo=timezone.utc),\n",
    "        \"type\": \"non-fiction\",\n",
    "        \"copies\": 20,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Insert books\n",
    "books = [Book2(**data) for data in books_data]\n",
    "results = await Book2.insert_many(books)\n",
    "book_ids = [str(uid) for uid in results.inserted_ids]\n",
    "\n",
    "print(f\"✓ Inserted {len(book_ids)} books\")\n",
    "print(f\"Book IDs: {book_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618437d6",
   "metadata": {},
   "source": [
    "#### 3.2.4 Query Exercises: Reference Pattern (Beanie Aggregation Methods)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Use Beanie's aggregation framework instead of raw MQL dictionaries\n",
    "- Perform joins using structured aggregation methods\n",
    "- Calculate derived fields (totals, counts, averages)\n",
    "- Compare performance trade-offs between embedding and referencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd777d",
   "metadata": {},
   "source": [
    "**Understanding Beanie Aggregation Methods**\n",
    "\n",
    "Beanie provides **two types of aggregation interfaces**:\n",
    "\n",
    "**1. Built-in Aggregation Methods** (Simple operations on query results)\n",
    "```python\n",
    "# Works on FindMany query results\n",
    "avg_price = await Product.find(Product.price > 10).avg(Product.price)\n",
    "sum_copies = await Book.find(Book.type == \"fiction\").sum(Book.copies)\n",
    "max_age = await Author.find().max(Author.age)\n",
    "```\n",
    "\n",
    "Available methods: `.sum()`, `.avg()`, `.max()`, `.min()`\n",
    "\n",
    "**2. Complex Aggregation Pipelines** (Using raw PyMongo syntax)\n",
    "\n",
    "For joins (`$lookup`), grouping (`$group`), and complex transformations, Beanie uses **raw PyMongo aggregation pipeline dictionaries**:\n",
    "\n",
    "```python\n",
    "# Beanie's aggregate() method accepts PyMongo pipeline syntax\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\"from\": \"book\", \"localField\": \"_id\", \"foreignField\": \"authors\", \"as\": \"books\"}},\n",
    "    {\"$project\": {\"name\": 1, \"book_count\": {\"$size\": \"$books\"}}}\n",
    "]\n",
    "\n",
    "results = await Author.aggregate(pipeline).to_list()\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- Beanie does **NOT** have special wrapper methods for `$lookup`, `$group`, `$project`, etc.\n",
    "- You write the **same MongoDB aggregation operators** as you would in PyMongo\n",
    "- The only difference: use `await Document.aggregate([pipeline])` instead of `collection.aggregate([pipeline])`\n",
    "- Use `projection_model` parameter to automatically parse results into Pydantic models\n",
    "\n",
    "**Reference:** [Beanie Aggregation Docs](https://beanie-odm.dev/tutorial/aggregation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f26aef",
   "metadata": {},
   "source": [
    "**Exercise 1: Join Authors with Their Books (Basic $lookup)**\n",
    "\n",
    "**Goal:** Retrieve all authors with their associated books using Beanie aggregation.\n",
    "\n",
    "**Requirements:**\n",
    "- Use Beanie's aggregation methods (NOT raw pipeline dicts)\n",
    "- Join `Author` collection with `Book` collection\n",
    "- Include: `first_name`, `last_name`, and list of book titles\n",
    "\n",
    "**Expected Output Structure:**\n",
    "```json\n",
    "{\n",
    "  \"first_name\": \"Haruki\",\n",
    "  \"last_name\": \"Murakami\",\n",
    "  \"books\": [\n",
    "    {\"title\": \"Kafka on the Shore\"},\n",
    "    {\"title\": \"Norwegian Wood\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Hints:**\n",
    "- Use `Author.aggregate()` with aggregation pipeline\n",
    "- `$lookup` stage connects collections: `{\"$lookup\": {\"from\": \"book\", \"localField\": \"_id\", \"foreignField\": \"authors\", \"as\": \"books\"}}`\n",
    "- `$project` stage to select only needed fields\n",
    "- Use `$map` inside project to transform books array to only show titles\n",
    "- Beanie method: `await Author.aggregate([...]).to_list()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36269853",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    # Direct ObjectId to ObjectId join - no conversion needed!\n",
    "    {\n",
    "        \"$lookup\": {\n",
    "            \"from\": \"Book2\",  # Fixed: MongoDB collection names are case-sensitive!\n",
    "            \"localField\": \"_id\",\n",
    "            \"foreignField\": \"authors\",\n",
    "            \"as\": \"books\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"first_name\": 1,\n",
    "            \"last_name\": 1,\n",
    "            \"books\": {\n",
    "                \"title\": 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "results = await Author2.aggregate(pipeline).to_list()\n",
    "printer.pprint([result for result in results])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ea6557",
   "metadata": {},
   "source": [
    "**Exercise 2: Calculate Total Books Per Author**\n",
    "\n",
    "**Goal:** For each author, calculate:\n",
    "- Total number of books\n",
    "- Total copies across all their books\n",
    "- Average copies per book\n",
    "\n",
    "**Expected Output:**\n",
    "```json\n",
    "{\n",
    "  \"author\": \"Haruki Murakami\",\n",
    "  \"total_books\": 2,\n",
    "  \"total_copies\": 21,\n",
    "  \"avg_copies_per_book\": 10.5\n",
    "}\n",
    "```\n",
    "\n",
    "**Hints:**\n",
    "- Start with `$lookup` to join Author with Book (from Exercise 1)\n",
    "- Use `$addFields` to add calculated fields\n",
    "- `$size` operator counts array elements: `{\"$size\": \"$books\"}`\n",
    "- `$sum` operator on array: `{\"$sum\": \"$books.copies\"}`\n",
    "- `$avg` operator: `{\"$avg\": \"$books.copies\"}`\n",
    "- Concatenate author name using `$concat`: `[\"$first_name\", \" \", \"$last_name\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24097597",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    {\n",
    "        \"$lookup\": {\n",
    "            \"from\": \"Book2\",\n",
    "            \"localField\": \"_id\",\n",
    "            \"foreignField\": \"authors\",\n",
    "            \"as\": \"books\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$addFields\": {\n",
    "            \"total_books\": {\"$size\": \"$books\"},\n",
    "            \"total_copies\": {\"$sum\": \"$books.copies\"},\n",
    "            \"avg_copies_per_book\": {\"$avg\": \"$books.copies\"},\n",
    "            \"author\": {\n",
    "                \"$concat\": [\n",
    "                    \"$first_name\",\n",
    "                    \" \",\n",
    "                    \"$last_name\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"author\": 1,\n",
    "            \"total_books\": 1,\n",
    "            \"total_copies\": 1,\n",
    "            \"avg_copies_per_book\": 1\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "results = await Author2.aggregate(pipeline).to_list()\n",
    "printer.pprint([result for result in results])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c35aff",
   "metadata": {},
   "source": [
    "**Exercise 3: Reverse Join - Books with Author Details**\n",
    "\n",
    "**Goal:** Start from `Book` collection and populate author information.\n",
    "\n",
    "**Requirements:**\n",
    "- For each book, include full author details (name, birth year)\n",
    "- Calculate author's age at publication time\n",
    "- Sort by publication date\n",
    "\n",
    "**Challenge:** Handle books with multiple authors.\n",
    "\n",
    "**Hints:**\n",
    "- Use `Book.aggregate()` starting point\n",
    "- `$lookup` with pipeline for complex join\n",
    "- `$dateDiff` to calculate age: `{\"$dateDiff\": {\"startDate\": \"$date_of_birth\", \"endDate\": \"$publish_date\", \"unit\": \"year\"}}`\n",
    "- `$sort` by publish_date: `{\"$sort\": {\"publish_date\": 1}}`\n",
    "- Use `$unwind` if you want one doc per author instead of array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd67d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Reverse join with calculated fields\n",
    "# Join from Book → Author, calculate age at publication\n",
    "\n",
    "pipeline = [\n",
    "\n",
    "    \n",
    "    # Stage 1: Lookup author details from Author2 collection\n",
    "    {\n",
    "        \"$lookup\": {\n",
    "            \"from\": \"Author2\",\n",
    "            \"localField\": \"authors\",\n",
    "            \"foreignField\": \"_id\",\n",
    "            \"as\": \"author_info\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Stage 2: Unwind the author_info array (will have 1 element per book)\n",
    "    {\n",
    "        \"$unwind\": {\n",
    "            \"path\": \"$author_info\",\n",
    "            \"preserveNullAndEmptyArrays\": True\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Stage 3: Add calculated fields\n",
    "    {\n",
    "        \"$addFields\": {\n",
    "            \"author_name\": {\n",
    "                \"$concat\": [\n",
    "                    \"$author_info.first_name\",\n",
    "                    \" \",\n",
    "                    \"$author_info.last_name\"\n",
    "                ]\n",
    "            },\n",
    "            \"author_birth_year\": {\"$year\": \"$author_info.date_of_birth\"},\n",
    "            \"age_at_publication\": {\n",
    "                \"$dateDiff\": {\n",
    "                    \"startDate\": \"$author_info.date_of_birth\",\n",
    "                    \"endDate\": \"$publish_date\",\n",
    "                    \"unit\": \"year\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Stage 4: Project only needed fields\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"title\": 1,\n",
    "            \"publish_date\": 1,\n",
    "            \"author_name\": 1,\n",
    "            \"author_birth_year\": 1,\n",
    "            \"age_at_publication\": 1\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Stage 6: Sort by publication date\n",
    "    {\n",
    "        \"$sort\": {\n",
    "            \"publish_date\": 1\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "results = await Book2.aggregate(pipeline).to_list()\n",
    "printer.pprint([result for result in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d741da",
   "metadata": {},
   "source": [
    "**Exercise 4: Filter Join - Books by Authors Born After 1970**\n",
    "\n",
    "**Goal:** Combine filtering with joining.\n",
    "\n",
    "**Requirements:**\n",
    "1. Join Books with Authors\n",
    "2. Filter to only include authors born after 1970\n",
    "3. Project: book title, author name, author birth year\n",
    "4. Sort by author birth year (newest first)\n",
    "\n",
    "**Expected Behavior:** Should exclude Murakami's books, include Adichie and Harari.\n",
    "\n",
    "**Hints:**\n",
    "- Start with `Book.aggregate()`\n",
    "- `$lookup` with `let` and pipeline for complex filtering\n",
    "- Inside lookup pipeline, use `$match` with `$expr` to filter by birth year\n",
    "- Extract year from date: `{\"$year\": \"$date_of_birth\"}`\n",
    "- Compare: `{\"$gt\": [{\"$year\": \"$date_of_birth\"}, 1970]}`\n",
    "- `$sort` by birth year descending: `{\"$sort\": {\"author_birth_year\": -1}}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Filter join by birth year\n",
    "# TODO: Combine $lookup with $match on nested fields\n",
    "\n",
    "pipeline = [\n",
    "    \n",
    "    # Stage 1: Lookup author details from Author2 collection\n",
    "    {\n",
    "        \"$lookup\": {\n",
    "            \"from\": \"Author2\",\n",
    "            \"localField\": \"authors\",\n",
    "            \"foreignField\": \"_id\",\n",
    "            \"as\": \"author_info\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"$unwind\": {\n",
    "            \"path\": \"$author_info\",\n",
    "            \"preserveNullAndEmptyArrays\": True\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"$match\": {\n",
    "            \"$expr\": {\n",
    "                \"$gt\": [\n",
    "                    {\"$year\": \"$author_info.date_of_birth\"},\n",
    "                    1970\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"$sort\": {\n",
    "            \"author_info.date_of_birth\": -1\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"title\": 1,\n",
    "            \"author_name\": {\n",
    "                \"$concat\": [\n",
    "                    \"$author_info.first_name\",\n",
    "                    \" \",\n",
    "                    \"$author_info.last_name\"\n",
    "                ]\n",
    "            },\n",
    "            \"author_birth_year\": {\"$year\": \"$author_info.date_of_birth\"},\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "results = await Book2.aggregate(pipeline).to_list()\n",
    "printer.pprint([result for result in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a336b0",
   "metadata": {},
   "source": [
    "**Exercise 5: Aggregated Statistics Across Relationship**\n",
    "\n",
    "**Goal:** Generate a report of book statistics grouped by fiction/non-fiction.\n",
    "\n",
    "**Requirements:**\n",
    "- Group books by `type` field\n",
    "- For each type, calculate:\n",
    "  - Total books\n",
    "  - Total copies in circulation\n",
    "  - List of unique authors (names only, deduplicated)\n",
    "  - Average publish year\n",
    "\n",
    "**Expected Output:**\n",
    "```json\n",
    "{\n",
    "  \"type\": \"fiction\",\n",
    "  \"total_books\": 4,\n",
    "  \"total_copies\": 34,\n",
    "  \"unique_authors\": [\"Haruki Murakami\", \"Chimamanda Ngozi Adichie\"],\n",
    "  \"avg_publish_year\": 1998\n",
    "}\n",
    "```\n",
    "\n",
    "**Hints:**\n",
    "- Start with `Book.aggregate()`\n",
    "- First `$lookup` to get author details\n",
    "- `$unwind` the authors array\n",
    "- `$group` by `type`: `{\"$group\": {\"_id\": \"$type\", ...}}`\n",
    "- Accumulators: `$sum: 1` for count, `$sum: \"$copies\"` for total copies\n",
    "- `$addToSet` for unique author names: `{\"$addToSet\": {\"$concat\": [\"$authors.first_name\", \" \", \"$authors.last_name\"]}}`\n",
    "- Extract year and average: `{\"$avg\": {\"$year\": \"$publish_date\"}}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913760ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Grouped statistics with deduplication\n",
    "# TODO: Use $group with $addToSet for unique authors\n",
    "\n",
    "pipeline = [\n",
    "    \n",
    "    # Stage 1: Lookup author details from Author2 collection\n",
    "    {\n",
    "        \"$lookup\": {\n",
    "            \"from\": \"Author2\",\n",
    "            \"localField\": \"authors\",\n",
    "            \"foreignField\": \"_id\",\n",
    "            \"as\": \"author_info\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"$unwind\": {\n",
    "            \"path\": \"$author_info\",\n",
    "            \"preserveNullAndEmptyArrays\": True\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$type\",\n",
    "            \"book_count\": {\"$sum\": 1},\n",
    "            \"total_copies\": {\"$sum\": \"$copies\"},\n",
    "            \"unique_authors\": {\n",
    "                \"$addToSet\": {\n",
    "                    \"$concat\": [\n",
    "                        \"$author_info.first_name\",\n",
    "                        \" \",\n",
    "                        \"$author_info.last_name\"\n",
    "                    ]\n",
    "                }          \n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"type\": \"$_id\",\n",
    "            \"book_count\": 1,\n",
    "            \"total_copies\": 1,\n",
    "            \"unique_authors\": 1\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "results = await Book2.aggregate(pipeline).to_list()\n",
    "printer.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7112f1a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 📚 Mental Models: `$map`, `$addFields`, and `$set`\n",
    "\n",
    "#### 1. **`$map` - The Array Transformer**\n",
    "\n",
    "**Mental Model:** Think of `$map` like Python's `map()` function or JavaScript's `.map()`. It's a **for loop that transforms each element in an array**.\n",
    "\n",
    "**Structure:**\n",
    "```javascript\n",
    "{\n",
    "  \"$map\": {\n",
    "    \"input\": \"$arrayField\",      // Which array to loop over\n",
    "    \"as\": \"variableName\",         // What to call each element (like 'item' in a for loop)\n",
    "    \"in\": <expression>            // What to do with each element (the transformation)\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Concrete Example:**\n",
    "```javascript\n",
    "// You have: authors: [ObjectId('abc'), ObjectId('def'), ObjectId('ghi')]\n",
    "// You want: author_ids_as_strings: ['abc', 'def', 'ghi']\n",
    "\n",
    "{\n",
    "  \"$addFields\": {\n",
    "    \"author_ids_as_strings\": {\n",
    "      \"$map\": {\n",
    "        \"input\": \"$authors\",           // Loop over the authors array\n",
    "        \"as\": \"author_id\",             // Call each ObjectId \"author_id\"\n",
    "        \"in\": { \"$toString\": \"$$author_id\" }  // Convert each one to string\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**⚠️ Key Syntax Rules:**\n",
    "- Inside `\"in\"`, use `$$variableName` (double `$$`) to reference the loop variable\n",
    "- Use `$fieldName` (single `$`) for fields from the main document\n",
    "- `$$this` is a special variable when you don't specify `\"as\"` (refers to current element)\n",
    "\n",
    "**When to use `$map`:**\n",
    "- ✅ Transforming arrays (extract a field, convert types, calculate values)\n",
    "- ✅ Building new arrays with different structure\n",
    "- ❌ NOT for filtering (use `$filter` instead)\n",
    "- ❌ NOT for flat list operations (use `$unwind` first)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **`$addFields` vs `$set` - The Great Confusion**\n",
    "\n",
    "**Short Answer:** **They are IDENTICAL.** `$set` is just an alias for `$addFields` introduced in MongoDB 4.2.\n",
    "\n",
    "| Feature | `$addFields` | `$set` |\n",
    "|---------|--------------|--------|\n",
    "| **Keeps existing fields** | ✅ Yes | ✅ Yes |\n",
    "| **Adds new fields** | ✅ Yes | ✅ Yes |\n",
    "| **Overwrites existing fields** | ✅ Yes | ✅ Yes |\n",
    "| **Syntax** | Same | Same |\n",
    "| **MongoDB Version** | 3.4+ | 4.2+ (newer) |\n",
    "\n",
    "**Why two names?**\n",
    "- `$addFields` - Original name (2016), describes what it does\n",
    "- `$set` - Newer alias (2019), shorter and matches SQL UPDATE syntax\n",
    "\n",
    "**Mental Model for Both:**\n",
    "Think of them as **\"Add or update these fields, keep everything else\"**\n",
    "\n",
    "**Example:**\n",
    "```javascript\n",
    "// Document BEFORE:\n",
    "{ title: \"1984\", author: \"Orwell\", pages: 328 }\n",
    "\n",
    "// Using $addFields or $set (IDENTICAL):\n",
    "{\n",
    "  \"$addFields\": {\n",
    "    \"category\": \"dystopian\",     // NEW field (adds it)\n",
    "    \"pages\": 350                 // EXISTING field (overwrites it)\n",
    "  }\n",
    "}\n",
    "\n",
    "// Document AFTER:\n",
    "{ title: \"1984\", author: \"Orwell\", pages: 350, category: \"dystopian\" }\n",
    "// Notice: title and author are STILL THERE (not removed)\n",
    "```\n",
    "\n",
    "**Compare with `$project`:**\n",
    "```javascript\n",
    "// Using $project (DIFFERENT - removes unlisted fields):\n",
    "{\n",
    "  \"$project\": {\n",
    "    \"title\": 1,\n",
    "    \"category\": \"dystopian\",\n",
    "    \"pages\": 350\n",
    "  }\n",
    "}\n",
    "\n",
    "// Document AFTER:\n",
    "{ title: \"1984\", pages: 350, category: \"dystopian\" }\n",
    "// Notice: \"author\" field is GONE! (not listed in $project)\n",
    "```\n",
    "\n",
    "**Recommendation:**\n",
    "- Use `$addFields` or `$set` (your choice, I prefer `$addFields` for clarity)\n",
    "- Never mix them in the same pipeline (pick one for consistency)\n",
    "- Use `$project` only when you want to **remove** fields\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Common `$map` Patterns**\n",
    "\n",
    "**Pattern 1: Extract a field from array of objects**\n",
    "```javascript\n",
    "// books: [{ title: \"Book1\", pages: 100 }, { title: \"Book2\", pages: 200 }]\n",
    "// Want: book_titles: [\"Book1\", \"Book2\"]\n",
    "\n",
    "{\n",
    "  \"$addFields\": {\n",
    "    \"book_titles\": {\n",
    "      \"$map\": {\n",
    "        \"input\": \"$books\",\n",
    "        \"as\": \"book\",\n",
    "        \"in\": \"$$book.title\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Pattern 2: Transform entire objects**\n",
    "```javascript\n",
    "// authors: [{ first: \"Jane\", last: \"Doe\" }, { first: \"John\", last: \"Smith\" }]\n",
    "// Want: author_names: [{ name: \"Jane Doe\" }, { name: \"John Smith\" }]\n",
    "\n",
    "{\n",
    "  \"$addFields\": {\n",
    "    \"author_names\": {\n",
    "      \"$map\": {\n",
    "        \"input\": \"$authors\",\n",
    "        \"as\": \"author\",\n",
    "        \"in\": {\n",
    "          \"name\": { \"$concat\": [\"$$author.first\", \" \", \"$$author.last\"] }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Pattern 3: Use document fields inside $map**\n",
    "```javascript\n",
    "// Calculate discount for each product based on customer type\n",
    "// customer_type: \"VIP\", products: [{ price: 100 }, { price: 200 }]\n",
    "\n",
    "{\n",
    "  \"$addFields\": {\n",
    "    \"discounted_products\": {\n",
    "      \"$map\": {\n",
    "        \"input\": \"$products\",\n",
    "        \"as\": \"product\",\n",
    "        \"in\": {\n",
    "          \"original\": \"$$product.price\",\n",
    "          \"discounted\": {\n",
    "            \"$cond\": [\n",
    "              { \"$eq\": [\"$customer_type\", \"VIP\"] },  // $ for document field\n",
    "              { \"$multiply\": [\"$$product.price\", 0.8] },  // $$ for loop variable\n",
    "              \"$$product.price\"\n",
    "            ]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Quick Decision Tree**\n",
    "\n",
    "**Do you need to transform an array?**\n",
    "- **YES** → Use `$map`\n",
    "  - Each element → different value: `\"in\": <simple expression>`\n",
    "  - Each element → object: `\"in\": { field1: ..., field2: ... }`\n",
    "\n",
    "**Do you need to add/update fields but keep existing ones?**\n",
    "- **YES** → Use `$addFields` (or `$set`)\n",
    "\n",
    "**Do you need to remove fields or only show specific fields?**\n",
    "- **YES** → Use `$project`\n",
    "\n",
    "**Do you need to work with each array element separately?**\n",
    "- **YES** → Use `$unwind` first, then `$addFields`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cccfdde",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ⚡ Performance: `$unwind` vs `$map` - The Critical Trade-off\n",
    "\n",
    "#### **The Fundamental Difference**\n",
    "\n",
    "**`$unwind` approach:**\n",
    "```javascript\n",
    "// 1 document with 3-element array → 3 separate documents\n",
    "{ book: \"Book1\", authors: [\"A\", \"B\", \"C\"] }\n",
    "     ↓ $unwind\n",
    "{ book: \"Book1\", authors: \"A\" }\n",
    "{ book: \"Book1\", authors: \"B\" }\n",
    "{ book: \"Book1\", authors: \"C\" }\n",
    "```\n",
    "\n",
    "**`$map` approach:**\n",
    "```javascript\n",
    "// Array stays as array, transformed in-place\n",
    "{ book: \"Book1\", authors: [\"A\", \"B\", \"C\"] }\n",
    "     ↓ $map\n",
    "{ book: \"Book1\", author_names: [\"Name A\", \"Name B\", \"Name C\"] }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Performance Comparison Matrix**\n",
    "\n",
    "| Factor | `$unwind` → Process → `$group` | `$map` (in-place) |\n",
    "|--------|-------------------------------|-------------------|\n",
    "| **Document Count** | 🔴 **Multiplies** (can be 10x-1000x) | 🟢 **Unchanged** |\n",
    "| **Memory Usage** | 🔴 **High** (doc explosion) | 🟢 **Low** (single doc) |\n",
    "| **Pipeline Complexity** | 🟡 **Multi-stage** (unwind → process → group) | 🟢 **Single stage** |\n",
    "| **Index Usage** | 🟢 **Can use indexes** after unwind | 🔴 **Limited** (can't index array elements efficiently) |\n",
    "| **Best For** | Filtering, joining, aggregating | Transforming, reshaping |\n",
    "| **CPU Usage** | 🔴 **Higher** (more docs to process) | 🟢 **Lower** |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Real Performance Impact: Example Scenario**\n",
    "\n",
    "**Scenario:** 1000 books, each with 3 authors on average\n",
    "\n",
    "**Approach 1: Unwind → Process → Group**\n",
    "```javascript\n",
    "[\n",
    "  { \"$unwind\": \"$authors\" },           // 1,000 → 3,000 documents\n",
    "  { \"$group\": { \"_id\": \"$type\", ... }} // Process 3,000 docs\n",
    "]\n",
    "```\n",
    "- **Intermediate documents:** 3,000 (3x increase)\n",
    "- **Memory impact:** ~3x more RAM needed\n",
    "- **Processing time:** ~2-4x slower for large datasets\n",
    "\n",
    "**Approach 2: Direct $map**\n",
    "```javascript\n",
    "[\n",
    "  { \n",
    "    \"$addFields\": {\n",
    "      \"author_names\": {\n",
    "        \"$map\": { \"input\": \"$authors\", \"in\": \"$$this.name\" }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "]\n",
    "```\n",
    "- **Intermediate documents:** 1,000 (no explosion)\n",
    "- **Memory impact:** Minimal\n",
    "- **Processing time:** Baseline\n",
    "\n",
    "---\n",
    "\n",
    "#### **When `$unwind` is BETTER (Despite Overhead)**\n",
    "\n",
    "✅ **1. When you need to filter by array elements**\n",
    "```javascript\n",
    "// Find books where AT LEAST ONE author is born after 1970\n",
    "[\n",
    "  { \"$unwind\": \"$authors\" },\n",
    "  { \"$match\": { \"authors.birth_year\": { \"$gt\": 1970 } } },\n",
    "  { \"$group\": { \"_id\": \"$_id\", \"authors\": { \"$push\": \"$authors\" } } }\n",
    "]\n",
    "\n",
    "// ❌ Can't do this efficiently with $map alone\n",
    "```\n",
    "\n",
    "✅ **2. When you need to join on array elements**\n",
    "```javascript\n",
    "// $lookup can join after unwind uses indexes\n",
    "[\n",
    "  { \"$unwind\": \"$author_ids\" },\n",
    "  { \"$lookup\": { \"from\": \"authors\", \"localField\": \"author_ids\", ... } }\n",
    "]\n",
    "```\n",
    "\n",
    "✅ **3. When you need separate aggregations per array element**\n",
    "```javascript\n",
    "// Group by author birth decade\n",
    "[\n",
    "  { \"$unwind\": \"$authors\" },\n",
    "  { \"$group\": { \"_id\": { \"$subtract\": [ \"$authors.birth_year\", { \"$mod\": [\"$authors.birth_year\", 10] } ] } } }\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **When `$map` is BETTER**\n",
    "\n",
    "✅ **1. Simple transformations (no filtering/grouping)**\n",
    "```javascript\n",
    "// Extract titles from books array\n",
    "{ \"$addFields\": { \"titles\": { \"$map\": { \"input\": \"$books\", \"in\": \"$$this.title\" } } } }\n",
    "```\n",
    "\n",
    "✅ **2. Type conversions or formatting**\n",
    "```javascript\n",
    "// Convert all author IDs to strings\n",
    "{ \"$addFields\": { \"author_ids_str\": { \"$map\": { \"input\": \"$authors\", \"in\": { \"$toString\": \"$$this._id\" } } } } }\n",
    "```\n",
    "\n",
    "✅ **3. Large arrays (performance critical)**\n",
    "```javascript\n",
    "// If array has 100+ elements, $unwind creates 100+ docs → slow\n",
    "// $map keeps it as 1 doc → fast\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Hybrid Approach: Best of Both Worlds**\n",
    "\n",
    "Sometimes you need both! Use `$map` for transformations, then `$unwind` only when necessary:\n",
    "\n",
    "**Example: Calculate average book count per author, but only for fiction books**\n",
    "\n",
    "❌ **Inefficient:**\n",
    "```javascript\n",
    "[\n",
    "  { \"$unwind\": \"$books\" },                    // Explode ALL books\n",
    "  { \"$match\": { \"books.type\": \"fiction\" } },  // Filter after explosion\n",
    "  { \"$group\": { \"_id\": \"$author\", \"count\": { \"$sum\": 1 } } }\n",
    "]\n",
    "```\n",
    "\n",
    "✅ **Efficient:**\n",
    "```javascript\n",
    "[\n",
    "  // Step 1: Filter array BEFORE unwinding (using $filter)\n",
    "  {\n",
    "    \"$addFields\": {\n",
    "      \"fiction_books\": {\n",
    "        \"$filter\": {\n",
    "          \"input\": \"$books\",\n",
    "          \"cond\": { \"$eq\": [\"$$this.type\", \"fiction\"] }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  // Step 2: NOW unwind (only fiction books)\n",
    "  { \"$unwind\": \"$fiction_books\" },\n",
    "  // Step 3: Group\n",
    "  { \"$group\": { \"_id\": \"$author\", \"count\": { \"$sum\": 1 } } }\n",
    "]\n",
    "```\n",
    "\n",
    "**Why it's better:**\n",
    "- Reduces documents to unwind (filters first)\n",
    "- Only explodes what you need\n",
    "\n",
    "---\n",
    "\n",
    "#### **Benchmarking Guidelines**\n",
    "\n",
    "**Small datasets (< 10,000 docs):**\n",
    "- Performance difference is negligible\n",
    "- Choose based on readability\n",
    "\n",
    "**Medium datasets (10,000 - 1,000,000 docs):**\n",
    "- `$unwind` overhead becomes noticeable (2-5x slower)\n",
    "- Use `$map` when possible\n",
    "\n",
    "**Large datasets (> 1,000,000 docs):**\n",
    "- `$unwind` can cause memory issues\n",
    "- MongoDB's 16MB document limit can be hit after unwinding\n",
    "- Strongly prefer `$map` or hybrid approaches\n",
    "\n",
    "**MongoDB Atlas Performance Tip:**\n",
    "- Use `$sample` to test pipelines on subset first\n",
    "- Check aggregation execution stats with `explain: true`\n",
    "\n",
    "---\n",
    "\n",
    "#### **Decision Flowchart**\n",
    "\n",
    "```\n",
    "Do you need to filter/match on array elements?\n",
    "├─ YES → Use $unwind\n",
    "│         (Accept the performance cost for correctness)\n",
    "└─ NO → Continue...\n",
    "\n",
    "Do you need to join ($lookup) on array elements?\n",
    "├─ YES → Use $unwind\n",
    "└─ NO → Continue...\n",
    "\n",
    "Do you need to group/aggregate by array element properties?\n",
    "├─ YES → Use $unwind\n",
    "└─ NO → Use $map\n",
    "          (Faster, less memory, simpler)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Real Example Comparison**\n",
    "\n",
    "Let's solve the same problem both ways:\n",
    "\n",
    "**Problem:** For each book type, get a count of total books\n",
    "\n",
    "**Method 1: Unwind + Group**\n",
    "```javascript\n",
    "// Documents: 5 books\n",
    "pipeline = [\n",
    "  { \"$group\": { \"_id\": \"$type\", \"count\": { \"$sum\": 1 } } }\n",
    "]\n",
    "// Intermediate docs: 5 (no unwind needed here!)\n",
    "// Result: [{ \"_id\": \"fiction\", \"count\": 3 }, { \"_id\": \"non-fiction\", \"count\": 2 }]\n",
    "```\n",
    "\n",
    "**Method 2: Group + Map (if you want more details)**\n",
    "```javascript\n",
    "// Want book counts AND titles\n",
    "pipeline = [\n",
    "  {\n",
    "    \"$group\": {\n",
    "      \"_id\": \"$type\",\n",
    "      \"books\": { \"$push\": \"$title\" }  // Collect titles\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"$addFields\": {\n",
    "      \"count\": { \"$size\": \"$books\" }  // Count in-place\n",
    "    }\n",
    "  }\n",
    "]\n",
    "// Intermediate docs: 5 (no explosion)\n",
    "// Result: [{ \"_id\": \"fiction\", \"count\": 3, \"books\": [\"Book1\", \"Book2\", \"Book3\"] }]\n",
    "```\n",
    "\n",
    "**Winner:** Method 2 (no unwind needed, gives more data)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Takeaway**\n",
    "\n",
    "> **`$unwind` is a necessary evil for certain operations (filtering, joining), but it has a real performance cost. Use `$map`, `$filter`, and `$reduce` whenever you can keep arrays intact. For large datasets, avoiding `$unwind` can mean the difference between a 2-second query and a 30-second query.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ded4dc9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.3 Pattern Comparison & Best Practices\n",
    "\n",
    "**Exercise 7: Performance Analysis**\n",
    "\n",
    "**Goal:** Understand the trade-offs between embedding and referencing.\n",
    "\n",
    "**Tasks:**\n",
    "1. **Measure Query Performance:**\n",
    "   - Time a query fetching books with embedded authors\n",
    "   - Time the same query using references + join\n",
    "   - Compare execution times\n",
    "\n",
    "2. **Analyze Storage:**\n",
    "   - Calculate total document size for embedded pattern\n",
    "   - Calculate total document size for reference pattern\n",
    "   - Identify duplication in embedded approach\n",
    "\n",
    "3. **Update Scenarios:**\n",
    "   - What happens when an author's name changes?\n",
    "   - Embedded: How many documents need updating?\n",
    "   - Referenced: How many documents need updating?\n",
    "\n",
    "**Discussion Questions:**\n",
    "- When would embedding be preferred over referencing?\n",
    "- What are the warning signs that you've chosen the wrong pattern?\n",
    "- How does the access pattern (read-heavy vs write-heavy) influence the choice?\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 8: Hybrid Pattern (Subset Pattern)**\n",
    "\n",
    "**Challenge:** Implement a hybrid approach where:\n",
    "- `Book` stores a **subset** of author data (name only)\n",
    "- Full `Author` details remain in separate collection\n",
    "- On book query, you get basic author info without a join\n",
    "- On author profile page, you fetch full details\n",
    "\n",
    "**Goal:** Balance between query performance and data consistency.\n",
    "\n",
    "**Requirements:**\n",
    "```python\n",
    "class Book(Document):\n",
    "    title: str\n",
    "    author_summary: List[Dict[str, str]]  # {\"name\": \"Haruki Murakami\"}\n",
    "    author_ids: List[str]  # For fetching full details when needed\n",
    "    # ... other fields\n",
    "```\n",
    "\n",
    "**Discussion:** What are the synchronization challenges? When is this worth the complexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78dda6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7: Performance comparison\n",
    "# TODO: Implement timing measurements for both patterns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d56fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8: Hybrid subset pattern implementation\n",
    "# TODO: Design and implement subset pattern with synchronization strategy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a648b8",
   "metadata": {},
   "source": [
    "## 4. Cleanup\n",
    "\n",
    "Close the database connection and clean up resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67536e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: close the connection\n",
    "connector.close()\n",
    "print(\"✓ MongoDB connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
